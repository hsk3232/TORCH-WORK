{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "254c74ab",
   "metadata": {},
   "source": [
    "# 파이토치 회귀(Vor.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7451e270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader # 메모리가 작은 컴퓨터에서 딥러닝을 원샷에 돌리는 것이 불가능함으로 잘라서 넣기 위함임.\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"D2Coding\"\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # 내가 작은 자원이라도 이용해서 사용하기 위한 방법론임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fc5f88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyton의 문법은 자바랑 다름\n",
    "# Dataset -> extends 상속임\n",
    "\n",
    "# 상속: 재사용성을 높이기 위해 상위 클래스의 기능을 하위 클래스에 가지도록 강제하는 것 / \n",
    "#       강한 바인딩이 걸림 / 계층이 가진 기능을 아래 클래스가 가짐 / 만약 하위 클래스가 상위와 상충되면 오버라이딩으로 재정의함\n",
    "# 인터페이스: 기본적으로 계약 관계 / 반드시 구현하거나 마커를 사용함.\n",
    "\n",
    "# 파이썬의 상속은 재사용과는 상관 없음.\n",
    "\n",
    "\n",
    "# CustomDatasetdms Dataset을 상속하고, 해당 클래스에서 필요한 메서드를 덮어 씁니다.\n",
    "\n",
    "class CustomDataset(Dataset): # 확장 구문 / 그 안의 특정 메서드를 내쪽에서 구현해야 함. / 파이선은 인터페이스 개념이 없음 \n",
    "    def __init__(slef, x_tensor, y_tensor): # 가장 소중한 것: \"생성자\" / 모든 클래스는 생성자부터 만들어야 함 / 참고: ( __ : 던던, 매직 메서드)\n",
    "        slef.x = x_tensor\n",
    "        slef.y = y_tensor\n",
    "        \n",
    "    #Dataset을 상속받기 위해 필요한 2개의 메서드 __getitem__ / __len__\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index]) # index의 값을 가져올 때 사용!!\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    \n",
    " #이것만 하면 데이터 set을 만들 수 있음\n",
    " # 애를 왜써야 할까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58a95fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = 2\n",
    "true_b = 1\n",
    "N = 100\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(N, 1)\n",
    "epsilon = 0.1 * np.random.randn(N, 1) # 적당히 작은 노이즈 추가\n",
    "y = true_b + (true_w * x) + epsilon # 우리가 만든 정답지\n",
    "\n",
    "\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "train_idx = idx[:int(N *0.8)]\n",
    "val_idx = idx[int(N*0.8) :]\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]\n",
    "\n",
    "x_train_tensor = torch.as_tensor(x_train).float().to(device)\n",
    "y_train_tensor = torch.as_tensor(y_train).float().to(device)\n",
    "\n",
    "dataset = CustomDataset(x_train_tensor, y_train_tensor)\n",
    "\n",
    "ratio = .8\n",
    "n_total = len(dataset)\n",
    "n_train = int(n_total * ratio)\n",
    "n_val = n_total - n_train\n",
    "\n",
    "train_data, val_data = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True) # 로드 데이터를 불러 옴.\n",
    "val_loader = DataLoader(dataset=train_data, batch_size=16) # 로드 데이터를 불러 옴.\n",
    "\n",
    "# train_data = TensorDataset(x_train_tensor, y_train_tensor) 이거 쓰면 iris, 타이타닉 등 사용 안됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e24560d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "torch.manual_seed(42)\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device) # 레이어가 128개 늘어날 텐데 이것도 클래스 같은 것을 써서 관리 하겠구나.. 생각해야 함. \n",
    "optimizer = optim.SGD(model.parameters(), lr = lr)\n",
    "loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "# for epoch in range(n_epochs):\n",
    "#     model.train()\n",
    "#     yhat = model(x_train_tensor)\n",
    "#     loss = loss_fn(yhat, y_train_tensor)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "# print(model.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50154f51",
   "metadata": {},
   "source": [
    "- 학습용\n",
    "     - 모델은 계속해서 업데이트 하고, 검증은 동일해야 효율이 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "045dd5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_step_fn(model, loss_fn, optimizer): # 안쪽의 함수를 내 뱉으니까 \n",
    "    def perform_train_step_fn(x, y):\n",
    "        model.train()\n",
    "        yhat = model(x)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss.item()\n",
    "    return perform_train_step_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359e7e98",
   "metadata": {},
   "source": [
    "- 검증용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a461d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_val_step_fn(model, loss_fn):\n",
    "    def perform_val_step_fn(x, y):\n",
    "        model.eval()\n",
    "        yhat = model(x)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        return loss.item()\n",
    "    return perform_val_step_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf55d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch(device, data_loader, step_fn):\n",
    "    mini_betch_losses = []\n",
    "    for x_batch, y_batch in data_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        mini_batch_loss = step_fn(x_batch, y_batch)\n",
    "        mini_betch_losses.append(mini_batch_loss)\n",
    "    loss = np.mean(mini_betch_losses)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87232306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.state_dict of Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "losses = []\n",
    "train_setp_fn = make_train_step_fn(model, loss_fn, optimizer)\n",
    "for epoch in range(n_epochs):\n",
    "    loss = train_setp_fn(x_train_tensor, y_train_tensor)\n",
    "    losses.append(loss)\n",
    "    \n",
    "print(model.state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71eed057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[1.9582]])), ('0.bias', tensor([1.0274]))])\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 200\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_setp_fn = make_train_step_fn(model, loss_fn, optimizer)\n",
    "val_setp_fn = make_val_step_fn(model, loss_fn)\n",
    "\n",
    "write = SummaryWriter(\"run/simple_linear_regression\")\n",
    "x_sample, y_sample = next(iter(train_loader))\n",
    "write.add_graph(model, x_sample.to(device))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # loss = train_setp_fn(x_train_tensor, y_train_tensor)\n",
    "    loss = mini_batch(device, train_loader, train_setp_fn)\n",
    "    losses.append(loss)\n",
    "    with torch.no_grad():\n",
    "        val_loss = mini_batch(device, val_loader, val_setp_fn)\n",
    "        val_losses.append(val_loss)\n",
    "    write.add_scalars(\n",
    "        main_tag=\"loss\",\n",
    "        tag_scalar_dict={\"training\":loss, \"val_loss\": val_loss},\n",
    "        global_step=epoch\n",
    "    )\n",
    "write.close()\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90925ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 7312), started 0:02:39 ago. (Use '!kill 7312' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-123e43d2bceaa787\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-123e43d2bceaa787\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TensorBoard 확장 로드\n",
    "%load_ext tensorboard\n",
    "\n",
    "# 로그 디렉터리 지정 후 실행\n",
    "%tensorboard --logdir run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2f299b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.7296],\n",
       "         [0.4938],\n",
       "         [0.5979],\n",
       "         [0.5427],\n",
       "         [0.0344],\n",
       "         [0.6075],\n",
       "         [0.3309],\n",
       "         [0.3745],\n",
       "         [0.1960],\n",
       "         [0.9696],\n",
       "         [0.6233],\n",
       "         [0.7722],\n",
       "         [0.1159],\n",
       "         [0.0885],\n",
       "         [0.8872],\n",
       "         [0.1196]]),\n",
       " tensor([[2.5751],\n",
       "         [1.9060],\n",
       "         [2.0407],\n",
       "         [2.2161],\n",
       "         [1.1831],\n",
       "         [2.4037],\n",
       "         [1.5427],\n",
       "         [1.7578],\n",
       "         [1.4393],\n",
       "         [2.8401],\n",
       "         [2.2940],\n",
       "         [2.4208],\n",
       "         [1.1603],\n",
       "         [1.0708],\n",
       "         [2.8708],\n",
       "         [1.3214]])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f99949",
   "metadata": {},
   "source": [
    "# 고차원 함수(HoF) 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac6d0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def square(x):\n",
    "#     return x **2\n",
    "\n",
    "# def cube(x):\n",
    "#     return x **3\n",
    "\n",
    "# def forth_power(x):\n",
    "#     return x**4\n",
    "\n",
    "# def generic_expon(x, expon):\n",
    "#     return x**expon\n",
    "\n",
    "# 위 처럼 만들지 않는다. \n",
    "\n",
    "\n",
    "# 캡슐링\n",
    "def exponentiation_builder(exponnent):\n",
    "    def skeleton_exponentiation(x):\n",
    "        return x ** exponnent\n",
    "    return skeleton_exponentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "569ec254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9765625"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returned_fn = exponentiation_builder(10)\n",
    "returned_fn(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
